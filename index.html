<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">


  <title>Time is precious: Self-Supervised Learning Beyond Images</title>
  <meta name="description" content="Tutorial in ECCV 2024">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Time is precious: Self-Supervised Learning Beyond Images"/>
  

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./css/main.css?1" media="screen,projection">

  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/ff6e9b10da.js" crossorigin="anonymous"></script>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
		<li><a href="#speakers">Speakers</a></li>
        <li><a href="#talks">Schedule</a></li>
        <li><a href="#organizers">About Us</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
  <center><h2><b>ECCV 2024 Tutorial:</b></h2></center><br>
    <center><h1>Time is precious: Self-Supervised Learning Beyond Images</h1></center><br>
        <center><h2><i>30th September, 09:00 to 13:00 CST, Amber 7 + 8 </i><h2>
        	<h2><i>MiCo Milano</i><h2>
        
        </center>
  </div>
</div>

<center>
<br>
<a>
    <img width="400px" src="./img/eccv_logo.png" />
</a>
<br>
<br>
<br>
<!-- <a>
    <img src="./img/ldm_figure.png" />
</a> -->
</center>

<!-- <br>
<p style="color:red;text-align:center">
<b> &#x1F4E2; Check <a href="https://drive.google.com/file/d/1p_aZ627Bwvku7nKyYRHtfZe50nAnpqGU/view?usp=sharing">this Google drive link</a> for our tutorial slides.</b>
</p>
<br> -->
<!-- <br>
<p style="color:red;text-align:center">
<b> &#x1F4E2; &#x1F3A5; Check <a href="https://drive.google.com/file/d/1DYHDbt1tSl9oqm3O333biRYzSCOtdtmn/view?usp=sharing">this Google drive link</a> for our tutorial slides and this <a href="https://www.youtube.com/watch?v=cS6JQpEY9cs">YouTube link</a> for the video recording.</b>
</p>
<br> -->

<br>
<h2>Overview</h2>
<br/>
<p>
SSL has allowed pretraining of neural networks to scale beyond the size of labelled datasets, demonstrating robust performance without the need for costly annotation. This approach has successfully scaled training dataset sizes to billions of images.
</p>
However, state-of-the-art (SoTA) models often learn representations limited to single-image inputs, lacking temporal context. Visual representations derived from such static images are limited to learning from disjoint static snapshots of the world. This limitation is particularly pronounced in recent SSL techniques, all of which are trained on meticulously curated and  object-centric datasets, such as ImageNet. Attempts to scale up single-image techniques to larger, less-curated datasets like Instagram-1B have not yielded substantial improvements in performance. A single image, regardless of artificial augmentation, has its constraints: it cannot create new perspectives of an object or anticipate unfolding events in a scene.
</p>
<p>
The primary goal of this tutorial is to introduce to the computer vision community, the concept of learning robust representations by leveraging the rich information in video frames. While, image-based pretraining has gained recent popularity with SimCLR, the practice of pretraining models from videos dates back much earlier. This tutorial will recapitulate both early and recent works, which have pretrained image encoders using videos for different pretext tasks such as egomotion prediction, active recognition, dense prediction etc. We also discuss practical implementation details relevant for practitioners and highlight connections to other, existing works such as VITO, TimeTuning, DoRA, V-JEPA etc.  We also discuss recent works aimed to mimic human visual systems such learning from one continuous video stream and by learning from longitudinal audio-visual headcam recordings from young children, thereby putting this concept into a broader context. 
</p>

<h2>Motivation</h2>
<br/>
<p>These works from the last 6-12 months demonstrate a paradigm shift, showcasing SSL models pretrained on videos can outperform image-based pretraining. The notable progress validates this trend, making this tutorial on past and present advancements crucial for newcomers in the field. Key questions we aim to tackle in this tutorial include:</p>
<ul>
    <li>Can we learn strong image encoders from good quality videos (i.e. with limited data)?</li>
    <li>Do we need synthetic augmentations? How useful are the natural augmentations in videos?</li>
    <li>Can we learn from a continuous stream similar to humans?</li>
</ul>


<br id="speakers" /><br/>


<div class="row">
  <div class="col-xs-12">
    <h2>Speakers</h2><br>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">


  <div class="row">
    <div class="col-xs-4" id="shashank">
      <center>
      <a href="https://shashankvkt.github.io/">
        <img class="people-pic" src="./img/people/shashank.jpg" />
      </a>
      <div class="people-name">
        <a href="https://shashankvkt.github.io/">Shashanka Venkataramanan</a>
        <h6>INRIA</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="mrzsalehi">
      <center>
      <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">
        <img class="people-pic" src="./img/people/mrzsalehi.jpeg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">Mohammadreza Salehi</a>
        <h6>University of Amsterdam</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="arash">
      <center>
      <a href="https://yukimasano.github.io/">
        <img class="people-pic" src="./img/people/yuki.jpg" />
      </a>
      <div class="people-name">
         <a href="https://yukimasano.github.io/">Yuki M. Asano</a>
        <h6>University of Amsterdam</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="joao">
      <center>
      <a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ&hl=en/">
        <img class="people-pic" src="./img/people/joao.jpeg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ&hl=en/">João Carreira</a>
        <h6>Google DeepMind</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="karsten">
      <center>
      <a href="https://imisra.github.io/">
        <img class="people-pic" src="./img/people/ishan.jpeg" />
      </a>
      <div class="people-name">
        <a href="https://imisra.github.io/">Ishan Misra</a>
        <h6>GenAI, Meta</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="karsten">
      <center>
      <a href="https://sites.google.com/view/eminorhan/">
        <img class="people-pic" src="./img/people/emin.jpeg" />
      </a>
      <div class="people-name">
        <a href="https://sites.google.com/view/eminorhan/">Emin Orhan</a>
        <h6>Independent Researcher</h6>
      </div>
    </center>
    </div>
  </div>

<!--   <div class="row">
	<center>
	    <img src="./img/nvidia_logo.png" style="width:30%" />
	    <img src="./img/googledeepmind_logo.png" style="width:40%" />
	</center>
  </div> -->

<br id="talks" />
<br>
<br>

<div class="row">
  <div class="col-xs-12">
     <h2>Schedule</h2>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
        <th>Title</th>
        <th>Speaker</th>
        <th>Time (CST)</th>

        <!-- <th>Duration</th> -->
        </tr>
      </thead>
      <tbody>

        <tr>
            <td>Introduction</td>
            <td><a href="https://shashankvkt.github.io/">Shashanka</a> </td>
            <td>09:00 - 09:10</td>
        </tr>
        <tr>
            <td>Part (1): Learning image encoders from videos<br>Prior works</td>
            <td><a href="https://shashankvkt.github.io/">Shashanka</a> & <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">Mohammadreza</a></td>
            <!-- <td>40 minutes</td> -->
            <td>09:10 - 09:50</td>
        </tr>
        <tr>
            <td>Part (2): Tuning DINO with videos, 1-video pretraining, tracking image-patches </td>
            <td><a href="https://yukimasano.github.io/">Yuki M. Asano</a></td>
            <!-- <td>40 minutes</td> -->
            <td>09:50 - 10:30</td>
        </tr> 
        <tr>
            <td><b>Coffee Break</b><br></td>
            <td></td>
            <td>10:30 - 11:00</td>
        </tr>

        <tr>
            <td>Applications (1): Learning from one continous stream: <br>single-stream continual learning, massively parallel video models, perceivers</td>
            <td><a href="https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ&hl=en/">João Carreira</a></td>
            <!-- <td>40 minutes</td> -->
            <td style="white-space: nowrap;">11:00 - 11:40</td>
        </tr>
        
         <tr>
            <td>Applications (2): What makes Generative video models tick?<br>Emu Video (text-to-video), FlowVid (video-to-video), factorizing text-to-video generation,  efficiency </td>
            <td><a href="https://imisra.github.io/">Ishan Misra</a></td>
            <!-- <td>40 minutes</td> -->
            <td>11:40 - 12:20</td>
        </tr>
        <tr>
            <td>Applications (3): SSL from the perspective of a developing child<br>Audio-visual dataset, development of early word learning, learning from children </td>
            <td><a href="https://sites.google.com/view/eminorhan/">Emin Orhan</a></td>
            <!-- <td>40 minutes</td> -->
            <td>12:20 - 13:00</td>
        </tr>
        <tr>
            <td>Conclusion, Open Problems & Final remarks </td>
            <td><a href="https://yukimasano.github.io/">Yuki M. Asano</a></td>
            <!-- <td>40 minutes</td> -->
            <td>13:00 - 13:10</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>



<br id="organizers" />
<br>
<br>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>About Us</h2>
  </div>
</div>

<div class="row speaker" id="shashanka">
  <div class="col-sm-3 speaker-pic">
    <a href="https://shashankvkt.github.io/">
      <img class="people-pic" src="./img/people/shashank.jpg" />
    </a>
    <div class="people-name">
      <a href="https://shashankvkt.github.io/">Shashanka Venkataramanan</a> <a href="https://twitter.com/shawshank_v"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>INRIA</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    Shashanka is a final year PhD student at the LinkMedia team in INRIA, France, advised by Yannis Avrithis. He conducts research on the topic on self-supervised learning, specifically on learning image representation from videos and data-augmentation methods.  He has organized several deep learning workshop focusing on a broad range of topics including diffusion models, RAG, backdoor attacks etc. in his university. 
    </p>
  </div>
</div>

<div class="row speaker" id="ruiqi">
  <div class="col-sm-3 speaker-pic">
    <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">
      <img class="people-pic" src="./img/people/mrzsalehi.jpeg" />
    </a>
    <div class="people-name">
      <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">Mohammadreza Salehi</a> <a href="https://twitter.com/MrzSalehi"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>QUVA Lab, University of Amsterdam</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    	Mohammadreza is a third year PhD student at the QUVA lab, University of Amsterdam advised by Yuki Asano, Cees Snoek, and Efstratios Gavves. His research focuses on representation learning, with a special emphasis on learning image representations from videos. In addition to his primary research, he is also deeply engaged in the field of machine learning safety, working towards ensuring that AI systems are reliable and safe for society. 
    </p>
  </div>
</div>



<div class="row speaker" id="arash">
  <div class="col-sm-3 speaker-pic">
    <a href="https://yukimasano.github.io/">
      <img class="people-pic" src="./img/people/yuki.jpg" />
    </a>
    <div class="people-name">
      <a href="https://yukimasano.github.io/">Yuki Asano</a> <a href="https://twitter.com/y_m_asano"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>QUVA Lab, University of Amsterdam</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
      Yuki M. Asano is an assistant professor at the Video & Image Sense (VIS) Lab at the University of Amsterdam and leads the Qualcomm-UvA (QUVA) lab. 
      He conducts research on the topic on self-supervised learning, multi-modal learning and augmentations, which has resulted in works such as GDT, SSB, SeLa or single-image pretraining and most recently the self-supervised learning from videos such as TimeTuning and DoRA. He has served as Area Chair for CVPR 22-24, ICLR 2023 and NeurIPS 2022. He has also organized several workshops such as SSLWIN in ECCV 2020, ECCV 2022, BigMAC in ICCV 2023.
    </p>
  </div>
</div>


<br />
</div></div>

      </div>
    </div>
                <div class="section text-gray" id="footer">
                <div class="container">

                    <div class="row">
                       <div class="col-sm-5">
                        </div>
                        <div class="col-sm-7">
                            <p><small>
                            Template by <a href="https://nvlabs.github.io/eccv2020-limited-labels-data-tutorial/" class="external"> Shalini De Mello</a>.</small></p>
                        </div>

                    </div>

                </div>
            </div>


    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
  </body>
</html>
